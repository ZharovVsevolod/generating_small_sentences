# Генерация коротких языковых последовательностей (имён) с помощью различных нейросетевых моделей
В данном репозитории представлено небольшое исследование и сравнение архитектур языковых нейронных сетей (_mamba_, _transformer_, _LSTM_) и результатов генерации коротких последовательностей (в данном случае - имён).  

Цель данного исследования - выяснить, насколько хорошо новая архитектура языковой модели, _mamba_, которая разрабатывалась для захвата большого контекста, справляется с генерацией коротких последовательностей. Сможет ли она преодолеть в качестве модель на архитектуре _transformer_, а также для насколько серьёзно обе эти архитектуры превосходят в генерации _LSTM_-модель.  

Для генерации исследовались маленькие модели, размер которых не превышает 150 000 параметров (кроме _LSTM_-модели, её размер рассматривался вплоть до 250 000 параметров).

В качестве референса для написания модели на _mamba_-архитектуре использовался [mamba-minimal](https://github.com/johnma2006/mamba-minimal).

## Краткие результаты
_Mamba_ отлично заучивает исходные последовательности, даже при таком небольшом количестве параметров (131к). При этом значение `VCA`-метрики у неё скачет от эпохи к эпохе.

_Transformer_ (138к параметров) несколько хуже запоминает исходные последовательности, совершая ошибки в последовательности букв, однако у него более стабильная и более высокая `VCA`-метрика, по сравнению с моделью на _mamba_-архитектуре.

_LSTM_, как и следовало ожидать, даже при вдвое большем размере (237к параметров), не может даже приблизиться к результатам к моделям на архитектурах _mamba_ и _transformer_.

## Использование и генерация имён
> ***В разработке***

Для загрузки весов модели (в формате _pytorch lightning_ `*.ckpt`) и кастомного посимвольного токенизатора необходимо сначала импортировать, а потом и загрузить модули:
```python
from gen_names.models.shell import Model_Lightning_Shell
from gen_names.data import CharTokenizer

tokenizer = CharTokenizer()
model = Model_Lightning_Shell.load_from_checkpoint("path/to/model/weigths.ckpt")
```
Для запуска генерации модели предусмотрены два класса. `BeamGenerator` реализует лучевой поиск при генерации модели, а `TextGenerator` - top-k и top-n генерацию. Рекомендуется использовать именно `TextGenerator`.   
Импортировать и использовать модуль `TextGenerator` можно следующим образом:
```python
from gen_names.generators import TextGenerator

gen = TextGenerator(model, tokenizer, banned_idx=[0, 1, 2, 30, 31])
name = gen.generate(
    phrase="Fa", 
    mode="top_n", 
    n=0.9,
    max_len=9, 
    max_repeat=2
)
```
Рассмотрим на примере `TextGenerator` гиперпараметры, которые можно использовать при создании и генерации текста:
| Параметр | Инициализация | Назначение | Значение по умолчанию |
| --- | --- | --- | --- |
| model | при создании | Модель, которая будет генерировать текст | -
| tokenizer | при создании | Токенизатор для перевода текста в токены и обратно | -
| banned_idx | при создании | Какие индексы токенов не будут сгенерированы моделью | [0]
| device | при создании | Использование генерации только на процессоре (`cpu`) или с помощью видеокарты (`cuda`) | cuda
| eos_token_id | при создании | Токен конца последовательности, после которого модель перестанет генерировать | 3
| phrase | при генерации | Фраза, с которой начнётся генерироваться последовательность. ***Обязательно*** должна начинаться либо с заглавной `F` для генерирования женского имени, либо с `M` для мужского. ***Обязательно*** все последующие буквы должны быть _прописными_ (подробнее про это в блоке `Данные`) | F
| mode | при генерации | Способ генерации. Может быть `top-k` или `top-n` | top-k
| k | при генерации | Значение k для `top-k` генерации | 5
| n | при генерации | Значение n для `top-n` генерации | 0.9
| max_len | при генерации | Максимальная длина последовательности, которая может быть сгенерирована (может быть сгенерирована меньше) | 9
| max_repeat | при генерации | Количество одинаковых символов, которые могут быть в последовательности. Если это целое число, то тогда в сгенерированной последовательности не может быть быть больше `max_repeat` одинаковых токенов | None
---
В данный момент различные результаты генерации можно посмотреть (и самим запустить с различными параметрами) в `tests/mld.ipynb`.

## Данные
Данные взяты с открытого источника [Data.gov](https://catalog.data.gov/dataset/baby-names-from-social-security-card-applications-national-data), официального сайта правительства Соединённых Штатов. В выборке представлены имя и пол ребёнка, зарегистрированного в **Social Security card applications** с 1880 года. 
> При запуске файла `train.py` данные должны автоматически скачаться в папку `datasets/` в корне репозитория, но если по каким-то причинам это не получилось, для повторения экспериментов необходимо самостоятельно скачать и разархивировать файл с данными с сайта по данной [ссылке](https://catalog.data.gov/dataset/baby-names-from-social-security-card-applications-national-data).

Все имена построены по следующему принципу, которого необходимо придерживаться и при эксплуатации моделей для генерации. Слово _всегда_ начинается с одного из двух возможных заглавных символов: `M`, если имя мужское, или `F`, если женское. Все последующие буквы _всегда_ являются прописными.  
Например:
- Fanna;
- Fjennefer;
- Mrichard;
- Mgeralt.

Соотвественно, при эксплуатации моделей, если необходимо сгенерировать мужское имя, на вход надо подать "`M`", а если, например, женское имя, которое должно начинаться на B - "`Fb`".

## Метрика оценки
Для оценки качества модели используется кастомная система оценки `VowelsConsonantsAlternation_Metric`, или, сокращённо, `VCA`.

Эта метрика для подсчёта числового значения читаемости слова считается через соотношения переходов гласных и согласных.  
Формула:
$$VCA = \frac{1}{len(word)} * \frac{sum(+)}{sum(-) + 1},$$

где 
- `len(word)` - длина слова;
- `sum(+)` - количество переходов с согласной буквы на гласную или наоборот;
- `sum(-)` - количество переходов с согласной буквы на согласную (или с гласной на гласную).

`VCA` примимает значение в диапазоне `[0, 1)`. 

Распределение этой метрики можно посмотреть на `images\true_names_vca.png`. Опытным путём установлено, что для читаемости слова значение метрики необходимо не менее `0.2`.

## Обучение моделей
Посмотреть интерактивные графики обучения различных моделей и динамическое изменение метрик и генерации имён можно на онлайн-платформе [Weights & Biases](https://wandb.ai/lost_in_thoughts/gen_names).

Важные графики, на которые стоит обратить внимание:
| Параметр | Описание |
| --- | --- |
| train_loss | Значение фукнции потерь на тренировочном наборе данных
| val_loss | Значение фукнции потерь на валидационном наборе данных
| train_charerr | Значение метрики CharErrorRate на тренировочном наборе данных
| val_charerr | Значение метрики CharErrorRate на валидационном наборе данных
| train_vca | Значение метрики VCA на тренировочном наборе данных
| val_vca | Значение метрики VCA на валидационном наборе данных

